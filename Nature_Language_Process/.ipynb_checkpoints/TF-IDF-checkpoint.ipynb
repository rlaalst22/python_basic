{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF - IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = \"The cat sat on my face I hate a cat\"\n",
    "d2 = \"The dog sat on my bed I love a dog\"\n",
    "\n",
    "from math import log10\n",
    "\n",
    "# document 내 토큰이 등장한 빈도수 계산\n",
    "def f(t,d):\n",
    "    return d.count(t)\n",
    "\n",
    "#tf 계산 \n",
    "def tf(t,d):\n",
    "    return 0.5 + 0.5*f(t,d)/max([f(w,d) for w in d])\n",
    "\n",
    "#idf 계산\n",
    "def idf(t,D):\n",
    "    numerator =len(D)\n",
    "    denominator = 1+ len([True for d in D if t in d])\n",
    "    return log10(numerator/denominator)\n",
    "\n",
    "#ti-idf 계산 \n",
    "def tfidf(t,d,D):\n",
    "    print(D)\n",
    "    print(t)\n",
    "    print(d)\n",
    "    print(tf(t,d))\n",
    "    print(idf(t,D))\n",
    "    print(tf(t,d)*idf(t,D))\n",
    "    print('===')\n",
    "    return tf(t,d)*idf(t,D)\n",
    "\n",
    "#공백을 기준으로 토큰화\n",
    "def tokenizer(d):\n",
    "    return d.split()\n",
    "\n",
    "#tfidf 계산\n",
    "def tfidfScorer(D):\n",
    "    tokenized_D = [tokenizer(d) for d in D]\n",
    "    result = []\n",
    "    for d in tokenized_D:\n",
    "        result.append([(t,tfidf(t,d,tokenized_D)) for t in d])\n",
    "    return result\n",
    "\n",
    "corpus = [d1,d2]\n",
    "\n",
    "for i,doc in enumerate(tfidfScorer(corpus)):\n",
    "    print('========== document[%d]=========='% i)\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "document_ls = [d1,d2,d2]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfid = vectorizer.fit_transform(document_ls)\n",
    "word2id = defaultdict((lambda:0))\n",
    "for idx, feature in enumerate(vectorizer.get_feature_names()):\n",
    "    word2id[feature] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "document_ls = [d1, d2, d2]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(document_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = defaultdict(lambda : 0)\n",
    "for idx, feature in enumerate(vectorizer.get_feature_names()):\n",
    "    word2id[feature] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bed</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>face</th>\n",
       "      <th>hate</th>\n",
       "      <th>love</th>\n",
       "      <th>my</th>\n",
       "      <th>on</th>\n",
       "      <th>sat</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.735448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.367724</td>\n",
       "      <td>0.367724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217184</td>\n",
       "      <td>0.217184</td>\n",
       "      <td>0.217184</td>\n",
       "      <td>0.217184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.344779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.689558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344779</td>\n",
       "      <td>0.267752</td>\n",
       "      <td>0.267752</td>\n",
       "      <td>0.267752</td>\n",
       "      <td>0.267752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.344779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.689558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344779</td>\n",
       "      <td>0.267752</td>\n",
       "      <td>0.267752</td>\n",
       "      <td>0.267752</td>\n",
       "      <td>0.267752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bed       cat       dog      face      hate      love        my  \\\n",
       "0  0.000000  0.735448  0.000000  0.367724  0.367724  0.000000  0.217184   \n",
       "1  0.344779  0.000000  0.689558  0.000000  0.000000  0.344779  0.267752   \n",
       "2  0.344779  0.000000  0.689558  0.000000  0.000000  0.344779  0.267752   \n",
       "\n",
       "         on       sat       the  \n",
       "0  0.217184  0.217184  0.217184  \n",
       "1  0.267752  0.267752  0.267752  \n",
       "2  0.267752  0.267752  0.267752  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "count_vect_df = pd.DataFrame(tfidf.todense(), columns=vectorizer.get_feature_names())\n",
    "count_vect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = 'The cat sat on my face. I hate a cat'\n",
    "d2 = 'The dog sat on my bed. i love a dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "document_ls = [d1,d2]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(document_ls)\n",
    "\n",
    "word2id = defaultdict(lambda : 0)\n",
    "for idx,feature in enumerate(vectorizer.get_feature_names()):\n",
    "    word2id[feature] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datafram 으로 변환하여 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bed</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>face</th>\n",
       "      <th>hate</th>\n",
       "      <th>love</th>\n",
       "      <th>my</th>\n",
       "      <th>on</th>\n",
       "      <th>sat</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.706006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353003</td>\n",
       "      <td>0.353003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251164</td>\n",
       "      <td>0.251164</td>\n",
       "      <td>0.251164</td>\n",
       "      <td>0.251164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.353003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.706006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353003</td>\n",
       "      <td>0.251164</td>\n",
       "      <td>0.251164</td>\n",
       "      <td>0.251164</td>\n",
       "      <td>0.251164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bed       cat       dog      face      hate      love        my  \\\n",
       "0  0.000000  0.706006  0.000000  0.353003  0.353003  0.000000  0.251164   \n",
       "1  0.353003  0.000000  0.706006  0.000000  0.000000  0.353003  0.251164   \n",
       "\n",
       "         on       sat       the  \n",
       "0  0.251164  0.251164  0.251164  \n",
       "1  0.251164  0.251164  0.251164  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "count_vect_df = pd.DataFrame(tfidf.todense(),columns = vectorizer.get_feature_names())\n",
    "count_vect_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF score 가 높은 순으로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cat', 'hate', 'face'], dtype='<U4')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "feature_array = np.array(vectorizer.get_feature_names())\n",
    "tfidf_sorting = np.argsort(tfidf[0].toarray()).flatten()[::-1]\n",
    "\n",
    "n=3\n",
    "top_n = feature_array[tfidf_sorting][:n]\n",
    "\n",
    "top_n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Rank 직접 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \"The FAANG stocks won’t see much more growth in the near future, according to Bill Studebaker, founder and Chief Investment Officer of Robo Global. \\\n",
    "Studebaker argues we are seeing a 'reallocation' that will continue from large-cap tech stocks into market-weight stocks. \\\n",
    "The FAANG stocks have had a rough few weeks, and have been hit hard since March 12. \\\n",
    "One FAANG to look out for, in the midst of all this, is Amazon, according to Studebaker. \\\n",
    "The stock market is seeing a 'reallocation' out of FAANG stocks, which are not where the smart money is, founder and Chief Investment Officer of Robo Global Bill Studebaker told Business Insider. \\\n",
    "The FAANG stocks (Facebook, Apple, Amazon, Netflix, Google) are all down considerably since March 12, a trend that accelerated when news of a massive Facebook data scandal broke, sending the tech-heavy Nasdaq into a downward frenzy. \\\n",
    "Investors are wondering what’s next. \\\n",
    "And what’s next isn’t good news for FAANG stock optimists, Studebaker thinks. 'This is a dead trade' for the next several months, he said. 'I wouldn’t expect there to be a lot of performance attribution coming from the FAANG stocks,' he added. That is, if the stock market is to see gains in the next several months, they will largely not come from the big tech companies. \\\n",
    "The market is seeing a 'reallocation out of large-cap technology, into other parts of the market,' he said. And this trend could continue for the foreseeable future. 'When you get these reallocation trades, a de-risking, this can go on for months and months.' The FAANG’s are pricey stocks, he said, pointing out that investors will 'factor in the law of big numbers,' he said. 'Just because they’re big cap doesn’t mean they’re safe,' he added. \\\n",
    "Still, he doesn’t necessarily think that investors are going to shift drastically into value stocks. 'With an increasingly favorable macro backdrop, you have strong growth demand.' \\\n",
    "Studebaker, who runs an artificial intelligence and robotics exchange-traded fund with $4 billion in assets under management, thinks that AI and robotics are better areas of growth. His ETF is up 27% in the past year, while the FAANG stocks are also largely up over that same span, even if they are down since March 12. \\\n",
    "While many point to artificial intelligence as an area that will be a boost to Google and Amazon, Studebaker doesn’t see that as a sign of significant growth for the FAANGs. He pointed out that 'eighty to ninety percent of their businesses are still search,' and that 'AI doesn’t really move the needle on the business.' He also said 'the revenue mix [attributable to AI] in those businesses are insignificant.' \\\n",
    "And while he’s not bullish on FAANG’s, he does say that the one FAANG to still watch out for is Amazon, simply because ecommerce still represents a small portion of the global retail market, giving the company room to grow.\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text: \n",
      "\n",
      "['The', 'FAANG', 'stocks', 'won', '’', 't', 'see', 'much', 'more', 'growth', 'in', 'the', 'near', 'future', ',', 'according', 'to', 'Bill', 'Studebaker', ',', 'founder', 'and', 'Chief', 'Investment', 'Officer', 'of', 'Robo', 'Global.', 'Studebaker', 'argues', 'we', 'are', 'seeing', 'a', \"'reallocation\", \"'\", 'that', 'will', 'continue', 'from', 'large-cap', 'tech', 'stocks', 'into', 'market-weight', 'stocks.', 'The', 'FAANG', 'stocks', 'have', 'had', 'a', 'rough', 'few', 'weeks', ',', 'and', 'have', 'been', 'hit', 'hard', 'since', 'March', '12.', 'One', 'FAANG', 'to', 'look', 'out', 'for', ',', 'in', 'the', 'midst', 'of', 'all', 'this', ',', 'is', 'Amazon', ',', 'according', 'to', 'Studebaker.', 'The', 'stock', 'market', 'is', 'seeing', 'a', \"'reallocation\", \"'\", 'out', 'of', 'FAANG', 'stocks', ',', 'which', 'are', 'not', 'where', 'the', 'smart', 'money', 'is', ',', 'founder', 'and', 'Chief', 'Investment', 'Officer', 'of', 'Robo', 'Global', 'Bill', 'Studebaker', 'told', 'Business', 'Insider.', 'The', 'FAANG', 'stocks', '(', 'Facebook', ',', 'Apple', ',', 'Amazon', ',', 'Netflix', ',', 'Google', ')', 'are', 'all', 'down', 'considerably', 'since', 'March', '12', ',', 'a', 'trend', 'that', 'accelerated', 'when', 'news', 'of', 'a', 'massive', 'Facebook', 'data', 'scandal', 'broke', ',', 'sending', 'the', 'tech-heavy', 'Nasdaq', 'into', 'a', 'downward', 'frenzy.', 'Investors', 'are', 'wondering', 'what', '’', 's', 'next.', 'And', 'what', '’', 's', 'next', 'isn', '’', 't', 'good', 'news', 'for', 'FAANG', 'stock', 'optimists', ',', 'Studebaker', 'thinks.', \"'This\", 'is', 'a', 'dead', 'trade', \"'\", 'for', 'the', 'next', 'several', 'months', ',', 'he', 'said.', \"'\", 'I', 'wouldn', '’', 't', 'expect', 'there', 'to', 'be', 'a', 'lot', 'of', 'performance', 'attribution', 'coming', 'from', 'the', 'FAANG', 'stocks', ',', \"'\", 'he', 'added.', 'That', 'is', ',', 'if', 'the', 'stock', 'market', 'is', 'to', 'see', 'gains', 'in', 'the', 'next', 'several', 'months', ',', 'they', 'will', 'largely', 'not', 'come', 'from', 'the', 'big', 'tech', 'companies.', 'The', 'market', 'is', 'seeing', 'a', \"'reallocation\", 'out', 'of', 'large-cap', 'technology', ',', 'into', 'other', 'parts', 'of', 'the', 'market', ',', \"'\", 'he', 'said.', 'And', 'this', 'trend', 'could', 'continue', 'for', 'the', 'foreseeable', 'future.', \"'When\", 'you', 'get', 'these', 'reallocation', 'trades', ',', 'a', 'de-risking', ',', 'this', 'can', 'go', 'on', 'for', 'months', 'and', 'months.', \"'\", 'The', 'FAANG', '’', 's', 'are', 'pricey', 'stocks', ',', 'he', 'said', ',', 'pointing', 'out', 'that', 'investors', 'will', \"'factor\", 'in', 'the', 'law', 'of', 'big', 'numbers', ',', \"'\", 'he', 'said.', \"'Just\", 'because', 'they', '’', 're', 'big', 'cap', 'doesn', '’', 't', 'mean', 'they', '’', 're', 'safe', ',', \"'\", 'he', 'added.', 'Still', ',', 'he', 'doesn', '’', 't', 'necessarily', 'think', 'that', 'investors', 'are', 'going', 'to', 'shift', 'drastically', 'into', 'value', 'stocks.', \"'With\", 'an', 'increasingly', 'favorable', 'macro', 'backdrop', ',', 'you', 'have', 'strong', 'growth', 'demand.', \"'\", 'Studebaker', ',', 'who', 'runs', 'an', 'artificial', 'intelligence', 'and', 'robotics', 'exchange-traded', 'fund', 'with', '$', '4', 'billion', 'in', 'assets', 'under', 'management', ',', 'thinks', 'that', 'AI', 'and', 'robotics', 'are', 'better', 'areas', 'of', 'growth.', 'His', 'ETF', 'is', 'up', '27', '%', 'in', 'the', 'past', 'year', ',', 'while', 'the', 'FAANG', 'stocks', 'are', 'also', 'largely', 'up', 'over', 'that', 'same', 'span', ',', 'even', 'if', 'they', 'are', 'down', 'since', 'March', '12.', 'While', 'many', 'point', 'to', 'artificial', 'intelligence', 'as', 'an', 'area', 'that', 'will', 'be', 'a', 'boost', 'to', 'Google', 'and', 'Amazon', ',', 'Studebaker', 'doesn', '’', 't', 'see', 'that', 'as', 'a', 'sign', 'of', 'significant', 'growth', 'for', 'the', 'FAANGs.', 'He', 'pointed', 'out', 'that', \"'eighty\", 'to', 'ninety', 'percent', 'of', 'their', 'businesses', 'are', 'still', 'search', ',', \"'\", 'and', 'that', \"'AI\", 'doesn', '’', 't', 'really', 'move', 'the', 'needle', 'on', 'the', 'business.', \"'\", 'He', 'also', 'said', \"'the\", 'revenue', 'mix', '[', 'attributable', 'to', 'AI', ']', 'in', 'those', 'businesses', 'are', 'insignificant.', \"'\", 'And', 'while', 'he', '’', 's', 'not', 'bullish', 'on', 'FAANG', '’', 's', ',', 'he', 'does', 'say', 'that', 'the', 'one', 'FAANG', 'to', 'still', 'watch', 'out', 'for', 'is', 'Amazon', ',', 'simply', 'because', 'ecommerce', 'still', 'represents', 'a', 'small', 'portion', 'of', 'the', 'global', 'retail', 'market', ',', 'giving', 'the', 'company', 'room', 'to', 'grow', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\82102\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "nltk.download('punkt')\n",
    "text = TreebankWordTokenizer().tokenize(Text)\n",
    "\n",
    "print('Tokenized Text: \\n')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('FAANG', 'NNP'), ('stocks', 'NNS'), ('won', 'VBD'), ('’', 'JJ'), ('t', 'NN'), ('see', 'VBP'), ('much', 'RB'), ('more', 'JJR'), ('growth', 'NN'), ('in', 'IN'), ('the', 'DT'), ('near', 'JJ'), ('future', 'NN'), (',', ','), ('according', 'VBG'), ('to', 'TO'), ('Bill', 'NNP'), ('Studebaker', 'NNP'), (',', ','), ('founder', 'NN'), ('and', 'CC'), ('Chief', 'NNP'), ('Investment', 'NNP'), ('Officer', 'NNP'), ('of', 'IN'), ('Robo', 'NNP'), ('Global.', 'NNP'), ('Studebaker', 'NNP'), ('argues', 'VBZ'), ('we', 'PRP'), ('are', 'VBP'), ('seeing', 'VBG'), ('a', 'DT'), (\"'reallocation\", 'NN'), (\"'\", 'POS'), ('that', 'WDT'), ('will', 'MD'), ('continue', 'VB'), ('from', 'IN'), ('large-cap', 'JJ'), ('tech', 'NN'), ('stocks', 'NNS'), ('into', 'IN'), ('market-weight', 'JJ'), ('stocks.', 'NN'), ('The', 'DT'), ('FAANG', 'NNP'), ('stocks', 'NNS'), ('have', 'VBP'), ('had', 'VBD'), ('a', 'DT'), ('rough', 'JJ'), ('few', 'JJ'), ('weeks', 'NNS'), (',', ','), ('and', 'CC'), ('have', 'VBP'), ('been', 'VBN'), ('hit', 'VBN'), ('hard', 'JJ'), ('since', 'IN'), ('March', 'NNP'), ('12.', 'CD'), ('One', 'NNP'), ('FAANG', 'NNP'), ('to', 'TO'), ('look', 'VB'), ('out', 'RP'), ('for', 'IN'), (',', ','), ('in', 'IN'), ('the', 'DT'), ('midst', 'NN'), ('of', 'IN'), ('all', 'PDT'), ('this', 'DT'), (',', ','), ('is', 'VBZ'), ('Amazon', 'NNP'), (',', ','), ('according', 'VBG'), ('to', 'TO'), ('Studebaker.', 'NNP'), ('The', 'DT'), ('stock', 'NN'), ('market', 'NN'), ('is', 'VBZ'), ('seeing', 'VBG'), ('a', 'DT'), (\"'reallocation\", 'NN'), (\"'\", 'POS'), ('out', 'IN'), ('of', 'IN'), ('FAANG', 'NNP'), ('stocks', 'NNS'), (',', ','), ('which', 'WDT'), ('are', 'VBP'), ('not', 'RB'), ('where', 'WRB'), ('the', 'DT'), ('smart', 'JJ'), ('money', 'NN'), ('is', 'VBZ'), (',', ','), ('founder', 'NN'), ('and', 'CC'), ('Chief', 'NNP'), ('Investment', 'NNP'), ('Officer', 'NNP'), ('of', 'IN'), ('Robo', 'NNP'), ('Global', 'NNP'), ('Bill', 'NNP'), ('Studebaker', 'NNP'), ('told', 'VBD'), ('Business', 'NNP'), ('Insider.', 'NNP'), ('The', 'DT'), ('FAANG', 'NNP'), ('stocks', 'NNS'), ('(', '('), ('Facebook', 'NNP'), (',', ','), ('Apple', 'NNP'), (',', ','), ('Amazon', 'NNP'), (',', ','), ('Netflix', 'NNP'), (',', ','), ('Google', 'NNP'), (')', ')'), ('are', 'VBP'), ('all', 'DT'), ('down', 'RB'), ('considerably', 'RB'), ('since', 'IN'), ('March', 'NNP'), ('12', 'CD'), (',', ','), ('a', 'DT'), ('trend', 'NN'), ('that', 'WDT'), ('accelerated', 'VBD'), ('when', 'WRB'), ('news', 'NN'), ('of', 'IN'), ('a', 'DT'), ('massive', 'JJ'), ('Facebook', 'NNP'), ('data', 'NN'), ('scandal', 'NN'), ('broke', 'VBD'), (',', ','), ('sending', 'VBG'), ('the', 'DT'), ('tech-heavy', 'JJ'), ('Nasdaq', 'NNP'), ('into', 'IN'), ('a', 'DT'), ('downward', 'JJ'), ('frenzy.', 'NN'), ('Investors', 'NNS'), ('are', 'VBP'), ('wondering', 'VBG'), ('what', 'WP'), ('’', 'NNP'), ('s', 'VBD'), ('next.', 'DT'), ('And', 'CC'), ('what', 'WP'), ('’', 'NNP'), ('s', 'VBD'), ('next', 'JJ'), ('isn', 'NN'), ('’', 'NNP'), ('t', 'NN'), ('good', 'JJ'), ('news', 'NN'), ('for', 'IN'), ('FAANG', 'NNP'), ('stock', 'NN'), ('optimists', 'NNS'), (',', ','), ('Studebaker', 'NNP'), ('thinks.', 'NN'), (\"'This\", 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('dead', 'JJ'), ('trade', 'NN'), (\"'\", \"''\"), ('for', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('several', 'JJ'), ('months', 'NNS'), (',', ','), ('he', 'PRP'), ('said.', 'VBD'), (\"'\", \"''\"), ('I', 'PRP'), ('wouldn', 'VBP'), ('’', 'JJ'), ('t', 'NN'), ('expect', 'VBP'), ('there', 'EX'), ('to', 'TO'), ('be', 'VB'), ('a', 'DT'), ('lot', 'NN'), ('of', 'IN'), ('performance', 'NN'), ('attribution', 'NN'), ('coming', 'VBG'), ('from', 'IN'), ('the', 'DT'), ('FAANG', 'NNP'), ('stocks', 'NNS'), (',', ','), (\"'\", \"''\"), ('he', 'PRP'), ('added.', 'VBZ'), ('That', 'DT'), ('is', 'VBZ'), (',', ','), ('if', 'IN'), ('the', 'DT'), ('stock', 'NN'), ('market', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('see', 'VB'), ('gains', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('several', 'JJ'), ('months', 'NNS'), (',', ','), ('they', 'PRP'), ('will', 'MD'), ('largely', 'RB'), ('not', 'RB'), ('come', 'VBN'), ('from', 'IN'), ('the', 'DT'), ('big', 'JJ'), ('tech', 'NN'), ('companies.', 'VBZ'), ('The', 'DT'), ('market', 'NN'), ('is', 'VBZ'), ('seeing', 'VBG'), ('a', 'DT'), (\"'reallocation\", 'NN'), ('out', 'IN'), ('of', 'IN'), ('large-cap', 'JJ'), ('technology', 'NN'), (',', ','), ('into', 'IN'), ('other', 'JJ'), ('parts', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('market', 'NN'), (',', ','), (\"'\", \"''\"), ('he', 'PRP'), ('said.', 'VBZ'), ('And', 'CC'), ('this', 'DT'), ('trend', 'NN'), ('could', 'MD'), ('continue', 'VB'), ('for', 'IN'), ('the', 'DT'), ('foreseeable', 'JJ'), ('future.', 'NN'), (\"'When\", 'POS'), ('you', 'PRP'), ('get', 'VBP'), ('these', 'DT'), ('reallocation', 'NN'), ('trades', 'NNS'), (',', ','), ('a', 'DT'), ('de-risking', 'NN'), (',', ','), ('this', 'DT'), ('can', 'MD'), ('go', 'VB'), ('on', 'IN'), ('for', 'IN'), ('months', 'NNS'), ('and', 'CC'), ('months.', 'NN'), (\"'\", \"''\"), ('The', 'DT'), ('FAANG', 'NNP'), ('’', 'NNP'), ('s', 'NN'), ('are', 'VBP'), ('pricey', 'JJ'), ('stocks', 'NNS'), (',', ','), ('he', 'PRP'), ('said', 'VBD'), (',', ','), ('pointing', 'VBG'), ('out', 'RP'), ('that', 'IN'), ('investors', 'NNS'), ('will', 'MD'), (\"'factor\", 'VB'), ('in', 'IN'), ('the', 'DT'), ('law', 'NN'), ('of', 'IN'), ('big', 'JJ'), ('numbers', 'NNS'), (',', ','), (\"'\", \"''\"), ('he', 'PRP'), ('said.', 'VBZ'), (\"'Just\", 'CC'), ('because', 'IN'), ('they', 'PRP'), ('’', 'VBP'), ('re', 'JJ'), ('big', 'JJ'), ('cap', 'NN'), ('doesn', 'NN'), ('’', 'NNP'), ('t', 'NN'), ('mean', 'NN'), ('they', 'PRP'), ('’', 'VBP'), ('re', 'JJ'), ('safe', 'JJ'), (',', ','), (\"'\", \"''\"), ('he', 'PRP'), ('added.', 'VBZ'), ('Still', 'RB'), (',', ','), ('he', 'PRP'), ('doesn', 'VBZ'), ('’', 'JJ'), ('t', 'NNS'), ('necessarily', 'RB'), ('think', 'VBP'), ('that', 'IN'), ('investors', 'NNS'), ('are', 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('shift', 'VB'), ('drastically', 'RB'), ('into', 'IN'), ('value', 'NN'), ('stocks.', 'NN'), (\"'With\", 'CD'), ('an', 'DT'), ('increasingly', 'RB'), ('favorable', 'JJ'), ('macro', 'NN'), ('backdrop', 'NN'), (',', ','), ('you', 'PRP'), ('have', 'VBP'), ('strong', 'JJ'), ('growth', 'NN'), ('demand.', 'NN'), (\"'\", \"''\"), ('Studebaker', 'NNP'), (',', ','), ('who', 'WP'), ('runs', 'VBZ'), ('an', 'DT'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('and', 'CC'), ('robotics', 'NNS'), ('exchange-traded', 'JJ'), ('fund', 'NN'), ('with', 'IN'), ('$', '$'), ('4', 'CD'), ('billion', 'CD'), ('in', 'IN'), ('assets', 'NNS'), ('under', 'IN'), ('management', 'NN'), (',', ','), ('thinks', 'VBZ'), ('that', 'IN'), ('AI', 'NNP'), ('and', 'CC'), ('robotics', 'NNS'), ('are', 'VBP'), ('better', 'JJR'), ('areas', 'NNS'), ('of', 'IN'), ('growth.', 'NN'), ('His', 'PRP$'), ('ETF', 'NN'), ('is', 'VBZ'), ('up', 'RP'), ('27', 'CD'), ('%', 'NN'), ('in', 'IN'), ('the', 'DT'), ('past', 'JJ'), ('year', 'NN'), (',', ','), ('while', 'IN'), ('the', 'DT'), ('FAANG', 'NNP'), ('stocks', 'NNS'), ('are', 'VBP'), ('also', 'RB'), ('largely', 'RB'), ('up', 'IN'), ('over', 'IN'), ('that', 'DT'), ('same', 'JJ'), ('span', 'NN'), (',', ','), ('even', 'RB'), ('if', 'IN'), ('they', 'PRP'), ('are', 'VBP'), ('down', 'JJ'), ('since', 'IN'), ('March', 'NNP'), ('12.', 'CD'), ('While', 'IN'), ('many', 'JJ'), ('point', 'NN'), ('to', 'TO'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('as', 'IN'), ('an', 'DT'), ('area', 'NN'), ('that', 'WDT'), ('will', 'MD'), ('be', 'VB'), ('a', 'DT'), ('boost', 'NN'), ('to', 'TO'), ('Google', 'NNP'), ('and', 'CC'), ('Amazon', 'NNP'), (',', ','), ('Studebaker', 'NNP'), ('doesn', 'VBZ'), ('’', 'JJ'), ('t', 'NNS'), ('see', 'VBP'), ('that', 'IN'), ('as', 'IN'), ('a', 'DT'), ('sign', 'NN'), ('of', 'IN'), ('significant', 'JJ'), ('growth', 'NN'), ('for', 'IN'), ('the', 'DT'), ('FAANGs.', 'NNP'), ('He', 'PRP'), ('pointed', 'VBD'), ('out', 'RP'), ('that', 'IN'), (\"'eighty\", 'VBZ'), ('to', 'TO'), ('ninety', 'VB'), ('percent', 'NN'), ('of', 'IN'), ('their', 'PRP$'), ('businesses', 'NNS'), ('are', 'VBP'), ('still', 'RB'), ('search', 'RB'), (',', ','), (\"'\", \"''\"), ('and', 'CC'), ('that', 'IN'), (\"'AI\", 'NNP'), ('doesn', 'VBD'), ('’', 'NNP'), ('t', 'NN'), ('really', 'RB'), ('move', 'VB'), ('the', 'DT'), ('needle', 'NN'), ('on', 'IN'), ('the', 'DT'), ('business.', 'NN'), (\"'\", 'POS'), ('He', 'PRP'), ('also', 'RB'), ('said', 'VBD'), (\"'the\", 'JJ'), ('revenue', 'NN'), ('mix', 'NN'), ('[', 'NNP'), ('attributable', 'NN'), ('to', 'TO'), ('AI', 'NNP'), (']', 'NNP'), ('in', 'IN'), ('those', 'DT'), ('businesses', 'NNS'), ('are', 'VBP'), ('insignificant.', 'JJ'), (\"'\", 'POS'), ('And', 'CC'), ('while', 'IN'), ('he', 'PRP'), ('’', 'VBZ'), ('s', 'PRP'), ('not', 'RB'), ('bullish', 'VB'), ('on', 'IN'), ('FAANG', 'NNP'), ('’', 'NNP'), ('s', 'NN'), (',', ','), ('he', 'PRP'), ('does', 'VBZ'), ('say', 'VB'), ('that', 'IN'), ('the', 'DT'), ('one', 'CD'), ('FAANG', 'NNP'), ('to', 'TO'), ('still', 'RB'), ('watch', 'VB'), ('out', 'RP'), ('for', 'IN'), ('is', 'VBZ'), ('Amazon', 'NNP'), (',', ','), ('simply', 'RB'), ('because', 'IN'), ('ecommerce', 'NN'), ('still', 'RB'), ('represents', 'VBZ'), ('a', 'DT'), ('small', 'JJ'), ('portion', 'NN'), ('of', 'IN'), ('the', 'DT'), ('global', 'JJ'), ('retail', 'JJ'), ('market', 'NN'), (',', ','), ('giving', 'VBG'), ('the', 'DT'), ('company', 'NN'), ('room', 'NN'), ('to', 'TO'), ('grow', 'VB'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\82102\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "POS_tag = nltk.pos_tag(text)\n",
    "\n",
    "print(POS_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 표제어 추출(Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\82102\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.VERB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'FAANG', 'stock', 'win', '’', 't', 'see', 'much', 'more', 'growth', 'in', 'the', 'near', 'future', ',', 'accord', 'to', 'Bill', 'Studebaker', ',', 'founder', 'and', 'Chief', 'Investment', 'Officer', 'of', 'Robo', 'Global.', 'Studebaker', 'argue', 'we', 'be', 'see', 'a', \"'reallocation\", \"'\", 'that', 'will', 'continue', 'from', 'large-cap', 'tech', 'stock', 'into', 'market-weight', 'stocks.', 'The', 'FAANG', 'stock', 'have', 'have', 'a', 'rough', 'few', 'week', ',', 'and', 'have', 'be', 'hit', 'hard', 'since', 'March', '12.', 'One', 'FAANG', 'to', 'look', 'out', 'for', ',', 'in', 'the', 'midst', 'of', 'all', 'this', ',', 'be', 'Amazon', ',', 'accord', 'to', 'Studebaker.', 'The', 'stock', 'market', 'be', 'see', 'a', \"'reallocation\", \"'\", 'out', 'of', 'FAANG', 'stock', ',', 'which', 'be', 'not', 'where', 'the', 'smart', 'money', 'be', ',', 'founder', 'and', 'Chief', 'Investment', 'Officer', 'of', 'Robo', 'Global', 'Bill', 'Studebaker', 'tell', 'Business', 'Insider.', 'The', 'FAANG', 'stock', '(', 'Facebook', ',', 'Apple', ',', 'Amazon', ',', 'Netflix', ',', 'Google', ')', 'be', 'all', 'down', 'considerably', 'since', 'March', '12', ',', 'a', 'trend', 'that', 'accelerate', 'when', 'news', 'of', 'a', 'massive', 'Facebook', 'data', 'scandal', 'break', ',', 'send', 'the', 'tech-heavy', 'Nasdaq', 'into', 'a', 'downward', 'frenzy.', 'Investors', 'be', 'wonder', 'what', '’', 's', 'next.', 'And', 'what', '’', 's', 'next', 'isn', '’', 't', 'good', 'news', 'for', 'FAANG', 'stock', 'optimist', ',', 'Studebaker', 'thinks.', \"'This\", 'be', 'a', 'dead', 'trade', \"'\", 'for', 'the', 'next', 'several', 'month', ',', 'he', 'said.', \"'\", 'I', 'wouldn', '’', 't', 'expect', 'there', 'to', 'be', 'a', 'lot', 'of', 'performance', 'attribution', 'come', 'from', 'the', 'FAANG', 'stock', ',', \"'\", 'he', 'added.', 'That', 'be', ',', 'if', 'the', 'stock', 'market', 'be', 'to', 'see', 'gain', 'in', 'the', 'next', 'several', 'month', ',', 'they', 'will', 'largely', 'not', 'come', 'from', 'the', 'big', 'tech', 'companies.', 'The', 'market', 'be', 'see', 'a', \"'reallocation\", 'out', 'of', 'large-cap', 'technology', ',', 'into', 'other', 'part', 'of', 'the', 'market', ',', \"'\", 'he', 'said.', 'And', 'this', 'trend', 'could', 'continue', 'for', 'the', 'foreseeable', 'future.', \"'When\", 'you', 'get', 'these', 'reallocation', 'trade', ',', 'a', 'de-risking', ',', 'this', 'can', 'go', 'on', 'for', 'month', 'and', 'months.', \"'\", 'The', 'FAANG', '’', 's', 'be', 'pricey', 'stock', ',', 'he', 'say', ',', 'point', 'out', 'that', 'investor', 'will', \"'factor\", 'in', 'the', 'law', 'of', 'big', 'number', ',', \"'\", 'he', 'said.', \"'Just\", 'because', 'they', '’', 're', 'big', 'cap', 'doesn', '’', 't', 'mean', 'they', '’', 're', 'safe', ',', \"'\", 'he', 'added.', 'Still', ',', 'he', 'doesn', '’', 't', 'necessarily', 'think', 'that', 'investor', 'be', 'go', 'to', 'shift', 'drastically', 'into', 'value', 'stocks.', \"'With\", 'an', 'increasingly', 'favorable', 'macro', 'backdrop', ',', 'you', 'have', 'strong', 'growth', 'demand.', \"'\", 'Studebaker', ',', 'who', 'run', 'an', 'artificial', 'intelligence', 'and', 'robotics', 'exchange-traded', 'fund', 'with', '$', '4', 'billion', 'in', 'asset', 'under', 'management', ',', 'think', 'that', 'AI', 'and', 'robotics', 'be', 'good', 'area', 'of', 'growth.', 'His', 'ETF', 'be', 'up', '27', '%', 'in', 'the', 'past', 'year', ',', 'while', 'the', 'FAANG', 'stock', 'be', 'also', 'largely', 'up', 'over', 'that', 'same', 'span', ',', 'even', 'if', 'they', 'be', 'down', 'since', 'March', '12.', 'While', 'many', 'point', 'to', 'artificial', 'intelligence', 'a', 'an', 'area', 'that', 'will', 'be', 'a', 'boost', 'to', 'Google', 'and', 'Amazon', ',', 'Studebaker', 'doesn', '’', 't', 'see', 'that', 'a', 'a', 'sign', 'of', 'significant', 'growth', 'for', 'the', 'FAANGs.', 'He', 'point', 'out', 'that', \"'eighty\", 'to', 'ninety', 'percent', 'of', 'their', 'business', 'be', 'still', 'search', ',', \"'\", 'and', 'that', \"'AI\", 'doesn', '’', 't', 'really', 'move', 'the', 'needle', 'on', 'the', 'business.', \"'\", 'He', 'also', 'say', \"'the\", 'revenue', 'mix', '[', 'attributable', 'to', 'AI', ']', 'in', 'those', 'business', 'be', 'insignificant.', \"'\", 'And', 'while', 'he', '’', 's', 'not', 'bullish', 'on', 'FAANG', '’', 's', ',', 'he', 'do', 'say', 'that', 'the', 'one', 'FAANG', 'to', 'still', 'watch', 'out', 'for', 'be', 'Amazon', ',', 'simply', 'because', 'ecommerce', 'still', 'represent', 'a', 'small', 'portion', 'of', 'the', 'global', 'retail', 'market', ',', 'give', 'the', 'company', 'room', 'to', 'grow', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_text = []\n",
    "for word in POS_tag:\n",
    "    lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=get_wordnet_pos(word[1]))))\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FAANG', 'stock', 'win', 'more', 'growth', 'near', 'future', 'accord', 'Bill', 'Studebaker', 'founder', 'Chief', 'Investment', 'Officer', 'Robo', 'Global.', 'Studebaker', 'argue', \"'reallocation\", 'large-cap', 'tech', 'stock', 'market-weight', 'stocks.', 'FAANG', 'stock', 'rough', 'few', 'week', 'hard', 'March', 'One', 'FAANG', 'midst', 'Amazon', 'accord', 'Studebaker.', 'stock', 'market', \"'reallocation\", 'FAANG', 'stock', 'smart', 'money', 'founder', 'Chief', 'Investment', 'Officer', 'Robo', 'Global', 'Bill', 'Studebaker', 'tell', 'Business', 'Insider.', 'FAANG', 'stock', 'Facebook', 'Apple', 'Amazon', 'Netflix', 'Google', 'March', 'trend', 'accelerate', 'news', 'massive', 'Facebook', 'data', 'scandal', 'break', 'send', 'tech-heavy', 'Nasdaq', 'downward', 'frenzy.', 'Investors', 'wonder', 'next', 'good', 'news', 'FAANG', 'stock', 'optimist', 'Studebaker', 'thinks.', \"'This\", 'dead', 'trade', 'next', 'several', 'month', 'lot', 'performance', 'attribution', 'FAANG', 'stock', 'stock', 'market', 'gain', 'next', 'several', 'month', 'big', 'tech', 'market', \"'reallocation\", 'large-cap', 'technology', 'other', 'part', 'market', 'trend', 'foreseeable', 'future.', 'reallocation', 'trade', 'de-risking', 'month', 'months.', 'FAANG', 'pricey', 'stock', 'point', 'investor', 'law', 'big', 'number', 're', 'big', 'cap', 'mean', 're', 'safe', 'investor', 'value', 'stocks.', 'favorable', 'macro', 'backdrop', 'strong', 'growth', 'demand.', 'Studebaker', 'run', 'artificial', 'intelligence', 'robotics', 'exchange-traded', 'fund', 'asset', 'management', 'AI', 'robotics', 'good', 'area', 'growth.', 'ETF', 'past', 'year', 'FAANG', 'stock', 'same', 'span', 'March', 'many', 'point', 'artificial', 'intelligence', 'area', 'boost', 'Google', 'Amazon', 'Studebaker', 'sign', 'significant', 'growth', 'FAANGs.', 'point', 'percent', 'business', \"'AI\", 'needle', 'business.', \"'the\", 'revenue', 'mix', 'attributable', 'AI', 'business', 'insignificant.', 'FAANG', 'do', 'FAANG', 'Amazon', 'ecommerce', 'represent', 'small', 'portion', 'global', 'retail', 'market', 'give', 'company', 'room']\n"
     ]
    }
   ],
   "source": [
    "stopwords = []\n",
    "\n",
    "wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS']\n",
    "\n",
    "for word in POS_tag:\n",
    "    if word[1] not in wanted_POS:\n",
    "        stopwords.append(word[0])\n",
    "\n",
    "punctuations = list(str(string.punctuation))\n",
    "stopwords = stopwords + punctuations\n",
    "\n",
    "stopwords_plus = ['t','isn']\n",
    "stopwords = stopwords + stopwords_plus\n",
    "stopwords = set(stopwords)\n",
    "\n",
    "processed_text = []\n",
    "for word in lemmatized_text:\n",
    "    if word not in stopwords:\n",
    "        processed_text.append(word)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['near', 'FAANGs.', 'value', 'asset', 'accord', 'money', 'significant', 'tech-heavy', 'point', 'year', 'news', 'optimist', 'sign', 'tell', 'market', 'lot', 'dead', 'favorable', 'revenue', 'attribution', 'area', 'room', 'investor', 'frenzy.', 'portion', 'massive', 'big', 'technology', 'Robo', 'Amazon', 'hard', 'part', 'span', 'company', 'reallocation', 'Global', 'performance', 'next', 'demand.', 'represent', 'global', 'large-cap', 'good', 'Studebaker.', \"'reallocation\", 'stocks.', 'trend', 'argue', 'give', 'growth.', 'Netflix', 'pricey', 'fund', 'needle', 'trade', 'Global.', 'future.', 'Investors', \"'This\", 'law', 'mix', 'insignificant.', 'downward', 'strong', 'same', 'growth', 'past', 'ecommerce', 're', 'de-risking', 'backdrop', 'send', 'Business', 'several', 'scandal', 'AI', 'number', 'Apple', 'gain', 'Nasdaq', 'artificial', 'Studebaker', 'future', 'smart', \"'the\", 'safe', 'mean', 'Chief', 'months.', 'attributable', 'other', 'management', 'business.', 'Officer', 'win', 'break', 'month', 'Insider.', 'tech', 'more', 'foreseeable', 'ETF', 'FAANG', 'Investment', 'few', 'data', 'macro', 'One', 'Bill', 'thinks.', 'business', 'Google', 'cap', 'percent', 'market-weight', 'intelligence', 'exchange-traded', 'accelerate', 'many', 'wonder', 'stock', 'midst', 'rough', 'run', 'small', 'Facebook', 'robotics', 'retail', 'do', 'March', 'boost', 'week', 'founder', \"'AI\"]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(set(processed_text))\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그래프 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-8dce8a68ce06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindex_of_i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_of_j\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcovered_coocurrences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                         \u001b[0mweighted_edge\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_of_i\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mindex_of_j\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m                         \u001b[0mcovered_cooccurrences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_of_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_of_j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "vocab_len = len(vocabulary)\n",
    "\n",
    "weighted_edge = np.zeros((vocab_len,vocab_len),dtype = np.float32)\n",
    "\n",
    "score = np.zeros((vocab_len),dtype=np.float32)\n",
    "\n",
    "window_size =3\n",
    "covered_coocurrences = []\n",
    "\n",
    "for i in range(0,vocab_len):\n",
    "    score[i]=1\n",
    "    for j in range(0,vocab_len):\n",
    "        if j ==1:\n",
    "            continue\n",
    "        else:\n",
    "            for window_start in range(0,(len(processed_text)-window_size)):\n",
    "                window_end = window_start+window_size\n",
    "                window = processed_text[window_start:window_end]\n",
    "                \n",
    "                if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
    "                    index_of_i = window_start + window.index(vocabulary[i])\n",
    "                    index_of_j = window_start + window.index(vocabulary[j])\n",
    "                    \n",
    "                    if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n",
    "                        covered_cooccurrences.append([index_of_i, index_of_j])n                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "vocab_len = len(vocabulary)\n",
    "# 토큰별로 그래프 edge를 Matrix 형태로 생성\n",
    "weighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
    "# 각 토큰 노트별로 점수계산을 위한 배열 생성\n",
    "score = np.zeros((vocab_len),dtype=np.float32)\n",
    "# coocurrence를 판단하기 위한 window 사이즈 설정\n",
    "window_size = 3\n",
    "covered_coocurrences = []\n",
    "for i in range(0,vocab_len):\n",
    "    score[i]=1\n",
    "    for j in range(0,vocab_len):\n",
    "        if j==i:\n",
    "            continue\n",
    "        else:\n",
    "            for window_start in range(0,(len(processed_text)-window_size)):\n",
    "\n",
    "                window_end = window_start+window_size\n",
    "                window = processed_text[window_start:window_end]\n",
    "                # 탐색하고 있는 두 단어가 윈도(window)에 동시 등장할 경우\n",
    "\n",
    "                if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
    "                    index_of_i = window_start + window.index(vocabulary[i])\n",
    "                    index_of_j = window_start + window.index(vocabulary[j])\n",
    "                    \n",
    "                    if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n",
    "                        covered_coocurrences.append([index_of_i,index_of_j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 각 노드의 score 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "inout = np.zeros((vocab_len),dtype=np.float32)\n",
    "\n",
    "for i in range(0,vocab_len):\n",
    "    for j in range(0,vocab_len):\n",
    "        inout[i] += weighted_edge[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score ofnear:0.71418995\n",
      "Score ofFAANGs.:0.7059641\n",
      "Score ofvalue:0.7145285\n",
      "Score ofasset:0.808808\n",
      "Score ofaccord:1.2058179\n",
      "Score ofmoney:0.64826864\n",
      "Score ofsignificant:0.70546305\n",
      "Score oftech-heavy:0.9309464\n",
      "Score ofpoint:1.8006576\n",
      "Score ofyear:0.69595\n"
     ]
    }
   ],
   "source": [
    "MAX_ITERATIONS = 50\n",
    "d = 0.85\n",
    "threshold = 0.0001\n",
    "\n",
    "for iter in range(0,MAX_ITERATIONS):\n",
    "    prev_score = np.copy(score)\n",
    "    \n",
    "    for i in range(0,vocab_len):\n",
    "        \n",
    "        summation = 0\n",
    "        for j in range(0,vocab_len):\n",
    "            if weighted_edge[i][j] !=0:\n",
    "                summation += (weighted_edge[i][j]/inout[j]*score[j])\n",
    "            \n",
    "            score[i] = (1-d) + d*(summation)\n",
    "            \n",
    "        if np.sum(np.fabs(prev_score-score)) <= threshold:\n",
    "            break\n",
    "for i in range(0,10):\n",
    "    print('Score of'+vocabulary[i]+':'+str(score[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:\n",
      "\n",
      "FAANG:5.385965\n",
      "stock:5.301574\n",
      "Studebaker:3.3825405\n",
      "market:2.8711243\n",
      "Amazon:2.308305\n",
      "growth:1.9301474\n",
      "March:1.8372847\n",
      "next:1.8069803\n",
      "point:1.8006576\n",
      "big:1.7874424\n"
     ]
    }
   ],
   "source": [
    "sorted_index = np.flip(np.argsort(score),0)\n",
    "\n",
    "keywords_num = 10\n",
    "\n",
    "print('Keywords:\\n')\n",
    "\n",
    "for i in range(0,keywords_num):\n",
    "    print(str(vocabulary[sorted_index[i]])+':'+str(score[sorted_index[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank 핵심 구 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['FAANG', 'stock', 'win'], ['more', 'growth'], ['near', 'future'], ['accord'], ['Bill', 'Studebaker'], ['founder'], ['Chief', 'Investment', 'Officer'], ['Robo', 'Global.', 'Studebaker', 'argue'], [\"'reallocation\"], ['large-cap', 'tech', 'stock'], ['market-weight', 'stocks.'], ['FAANG', 'stock'], ['rough', 'few', 'week'], ['hard'], ['March'], ['One', 'FAANG'], ['midst'], ['Amazon'], ['accord'], ['Studebaker.'], ['stock', 'market'], [\"'reallocation\"], ['FAANG', 'stock'], ['smart', 'money'], ['founder'], ['Chief', 'Investment', 'Officer'], ['Robo', 'Global', 'Bill', 'Studebaker', 'tell', 'Business', 'Insider.'], ['FAANG', 'stock'], ['Facebook'], ['Apple'], ['Amazon'], ['Netflix'], ['Google'], ['March'], ['trend'], ['accelerate'], ['news'], ['massive', 'Facebook', 'data', 'scandal', 'break'], ['send'], ['tech-heavy', 'Nasdaq'], ['downward', 'frenzy.', 'Investors'], ['wonder'], ['next'], ['good', 'news'], ['FAANG', 'stock', 'optimist'], ['Studebaker', 'thinks.', \"'This\"], ['dead', 'trade'], ['next', 'several', 'month'], ['lot'], ['performance', 'attribution'], ['FAANG', 'stock'], ['stock', 'market'], ['gain'], ['next', 'several', 'month'], ['big', 'tech'], ['market'], [\"'reallocation\"], ['large-cap', 'technology'], ['other', 'part'], ['market'], ['trend'], ['foreseeable', 'future.'], ['reallocation', 'trade'], ['de-risking'], ['month'], ['months.'], ['FAANG'], ['pricey', 'stock'], ['point'], ['investor'], ['law'], ['big', 'number'], ['re', 'big', 'cap'], ['mean'], ['re', 'safe'], ['investor'], ['value', 'stocks.'], ['favorable', 'macro', 'backdrop'], ['strong', 'growth', 'demand.'], ['Studebaker'], ['run'], ['artificial', 'intelligence'], ['robotics', 'exchange-traded', 'fund'], ['asset'], ['management'], ['AI'], ['robotics'], ['good', 'area'], ['growth.'], ['ETF'], ['past', 'year'], ['FAANG', 'stock'], ['same', 'span'], ['March'], ['many', 'point'], ['artificial', 'intelligence'], ['area'], ['boost'], ['Google'], ['Amazon'], ['Studebaker'], ['sign'], ['significant', 'growth'], ['FAANGs.'], ['point'], ['percent'], ['business'], [\"'AI\"], ['needle'], ['business.'], [\"'the\", 'revenue', 'mix'], ['attributable'], ['AI'], ['business'], ['insignificant.'], ['FAANG'], ['do'], ['FAANG'], ['Amazon'], ['ecommerce'], ['represent'], ['small', 'portion'], ['global', 'retail', 'market'], ['give'], ['company', 'room']]\n"
     ]
    }
   ],
   "source": [
    "phrases = []\n",
    "\n",
    "phrase = ' '\n",
    "for word in lemmatized_text:\n",
    "    if word in stopwords:\n",
    "        if phrase != ' ':\n",
    "            phrases.append(str(phrase).strip().split())\n",
    "        phrase = ' '\n",
    "    elif word not in stopwords:\n",
    "        phrase +=str(word)\n",
    "        phrase += ' '\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['FAANG', 'stock', 'win'], ['more', 'growth'], ['near', 'future'], ['accord'], ['Bill', 'Studebaker'], ['founder'], ['Chief', 'Investment', 'Officer'], ['Robo', 'Global.', 'Studebaker', 'argue'], [\"'reallocation\"], ['large-cap', 'tech', 'stock'], ['market-weight', 'stocks.'], ['FAANG', 'stock'], ['rough', 'few', 'week'], ['hard'], ['March'], ['One', 'FAANG'], ['midst'], ['Amazon'], ['Studebaker.'], ['stock', 'market'], ['smart', 'money'], ['Robo', 'Global', 'Bill', 'Studebaker', 'tell', 'Business', 'Insider.'], ['Facebook'], ['Apple'], ['Netflix'], ['Google'], ['trend'], ['accelerate'], ['news'], ['massive', 'Facebook', 'data', 'scandal', 'break'], ['send'], ['tech-heavy', 'Nasdaq'], ['downward', 'frenzy.', 'Investors'], ['wonder'], ['next'], ['good', 'news'], ['FAANG', 'stock', 'optimist'], ['Studebaker', 'thinks.', \"'This\"], ['dead', 'trade'], ['next', 'several', 'month'], ['lot'], ['performance', 'attribution'], ['gain'], ['big', 'tech'], ['market'], ['large-cap', 'technology'], ['other', 'part'], ['foreseeable', 'future.'], ['reallocation', 'trade'], ['de-risking'], ['month'], ['months.'], ['FAANG'], ['pricey', 'stock'], ['point'], ['investor'], ['law'], ['big', 'number'], ['re', 'big', 'cap'], ['mean'], ['re', 'safe'], ['value', 'stocks.'], ['favorable', 'macro', 'backdrop'], ['strong', 'growth', 'demand.'], ['Studebaker'], ['run'], ['artificial', 'intelligence'], ['robotics', 'exchange-traded', 'fund'], ['asset'], ['management'], ['AI'], ['robotics'], ['good', 'area'], ['growth.'], ['ETF'], ['past', 'year'], ['same', 'span'], ['many', 'point'], ['area'], ['boost'], ['sign'], ['significant', 'growth'], ['FAANGs.'], ['percent'], ['business'], [\"'AI\"], ['needle'], ['business.'], [\"'the\", 'revenue', 'mix'], ['attributable'], ['insignificant.'], ['do'], ['ecommerce'], ['represent'], ['small', 'portion'], ['global', 'retail', 'market'], ['give'], ['company', 'room']]\n"
     ]
    }
   ],
   "source": [
    "unique_phrases = []\n",
    "\n",
    "for phrase in phrases:\n",
    "    if phrase not in unique_phrases:\n",
    "        unique_phrases.append(phrase)\n",
    "print(unique_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['FAANG', 'stock', 'win'], ['more', 'growth'], ['near', 'future'], ['accord'], ['Bill', 'Studebaker'], ['founder'], ['Chief', 'Investment', 'Officer'], ['Robo', 'Global.', 'Studebaker', 'argue'], [\"'reallocation\"], ['large-cap', 'tech', 'stock'], ['market-weight', 'stocks.'], ['FAANG', 'stock'], ['rough', 'few', 'week'], ['hard'], ['March'], ['One', 'FAANG'], ['midst'], ['Amazon'], ['Studebaker.'], ['stock', 'market'], ['smart', 'money'], ['Robo', 'Global', 'Bill', 'Studebaker', 'tell', 'Business', 'Insider.'], ['Apple'], ['Netflix'], ['Google'], ['trend'], ['accelerate'], ['massive', 'Facebook', 'data', 'scandal', 'break'], ['send'], ['tech-heavy', 'Nasdaq'], ['downward', 'frenzy.', 'Investors'], ['wonder'], ['good', 'news'], ['FAANG', 'stock', 'optimist'], ['Studebaker', 'thinks.', \"'This\"], ['dead', 'trade'], ['next', 'several', 'month'], ['lot'], ['performance', 'attribution'], ['gain'], ['big', 'tech'], ['large-cap', 'technology'], ['other', 'part'], ['foreseeable', 'future.'], ['reallocation', 'trade'], ['de-risking'], ['months.'], ['pricey', 'stock'], ['investor'], ['law'], ['big', 'number'], ['re', 'big', 'cap'], ['mean'], ['re', 'safe'], ['value', 'stocks.'], ['favorable', 'macro', 'backdrop'], ['strong', 'growth', 'demand.'], ['run'], ['artificial', 'intelligence'], ['robotics', 'exchange-traded', 'fund'], ['asset'], ['management'], ['AI'], ['good', 'area'], ['growth.'], ['ETF'], ['past', 'year'], ['same', 'span'], ['many', 'point'], ['boost'], ['sign'], ['significant', 'growth'], ['FAANGs.'], ['percent'], ['business'], [\"'AI\"], ['needle'], ['business.'], [\"'the\", 'revenue', 'mix'], ['attributable'], ['insignificant.'], ['do'], ['ecommerce'], ['represent'], ['small', 'portion'], ['global', 'retail', 'market'], ['give'], ['company', 'room']]\n"
     ]
    }
   ],
   "source": [
    "for word in vocabulary:\n",
    "    for phrase in unique_phrases:\n",
    "        if word in phrase and [word] in unique_phrases and len(phrase)>1:\n",
    "            unique_phrases.remove([word])\n",
    "print(unique_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 각 구의 score 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FAANG stock win', 'more growth', 'near future', 'accord', 'Bill Studebaker', 'founder', 'Chief Investment Officer', 'Robo Global. Studebaker argue', \"'reallocation\", 'large-cap tech stock', 'market-weight stocks.', 'FAANG stock', 'rough few week', 'hard', 'March', 'One FAANG', 'midst', 'Amazon', 'Studebaker.', 'stock market', 'smart money', 'Robo Global Bill Studebaker tell Business Insider.', 'Apple', 'Netflix', 'Google', 'trend', 'accelerate', 'massive Facebook data scandal break', 'send', 'tech-heavy Nasdaq', 'downward frenzy. Investors', 'wonder', 'good news', 'FAANG stock optimist', \"Studebaker thinks. 'This\", 'dead trade', 'next several month', 'lot', 'performance attribution', 'gain', 'big tech', 'large-cap technology', 'other part', 'foreseeable future.', 'reallocation trade', 'de-risking', 'months.', 'pricey stock', 'investor', 'law', 'big number', 're big cap', 'mean', 're safe', 'value stocks.', 'favorable macro backdrop', 'strong growth demand.', 'run', 'artificial intelligence', 'robotics exchange-traded fund', 'asset', 'management', 'AI', 'good area', 'growth.', 'ETF', 'past year', 'same span', 'many point', 'boost', 'sign', 'significant growth', 'FAANGs.', 'percent', 'business', \"'AI\", 'needle', 'business.', \"'the revenue mix\", 'attributable', 'insignificant.', 'do', 'ecommerce', 'represent', 'small portion', 'global retail market', 'give', 'company room']\n",
      "keyword: FAANG stock win, Score: 11.352636098861694\n",
      "keyword: more growth, Score: 2.6271471977233887\n",
      "keyword: near future, Score: 1.412111759185791\n",
      "keyword: accord, Score: 1.2058179378509521\n",
      "keyword: Bill Studebaker, Score: 4.563251852989197\n",
      "keyword: founder, Score: 1.143974781036377\n",
      "keyword: Chief Investment Officer, Score: 3.373182535171509\n",
      "keyword: Robo Global. Studebaker argue, Score: 5.8200682401657104\n",
      "keyword: 'reallocation, Score: 1.6229709386825562\n",
      "keyword: large-cap tech stock, Score: 7.580948114395142\n",
      "keyword: market-weight stocks., Score: 1.884601354598999\n",
      "keyword: FAANG stock, Score: 10.687539100646973\n",
      "keyword: rough few week, Score: 2.18459415435791\n",
      "keyword: hard, Score: 0.7367346882820129\n",
      "keyword: March, Score: 1.8372846841812134\n",
      "keyword: One FAANG, Score: 6.058057188987732\n",
      "keyword: midst, Score: 0.6467325091362\n",
      "keyword: Amazon, Score: 2.308305025100708\n",
      "keyword: Studebaker., Score: 0.6341211199760437\n",
      "keyword: stock market, Score: 8.172698497772217\n",
      "keyword: smart money, Score: 1.289451003074646\n",
      "keyword: Robo Global Bill Studebaker tell Business Insider., Score: 8.404916107654572\n",
      "keyword: Apple, Score: 0.6648016571998596\n",
      "keyword: Netflix, Score: 0.6655513644218445\n",
      "keyword: Google, Score: 1.2080210447311401\n",
      "keyword: trend, Score: 1.266885757446289\n",
      "keyword: accelerate, Score: 0.6904734969139099\n",
      "keyword: massive Facebook data scandal break, Score: 4.554100751876831\n",
      "keyword: send, Score: 0.920756459236145\n",
      "keyword: tech-heavy Nasdaq, Score: 1.8598706126213074\n",
      "keyword: downward frenzy. Investors, Score: 2.6357803344726562\n",
      "keyword: wonder, Score: 0.7714719176292419\n",
      "keyword: good news, Score: 2.47223699092865\n",
      "keyword: FAANG stock optimist, Score: 11.324375748634338\n",
      "keyword: Studebaker thinks. 'This, Score: 4.8242300152778625\n",
      "keyword: dead trade, Score: 2.0469314455986023\n",
      "keyword: next several month, Score: 4.752446889877319\n",
      "keyword: lot, Score: 0.6926352381706238\n",
      "keyword: performance attribution, Score: 1.3691638708114624\n",
      "keyword: gain, Score: 0.6427320241928101\n",
      "keyword: big tech, Score: 2.909052014350891\n",
      "keyword: large-cap technology, Score: 1.8480663895606995\n",
      "keyword: other part, Score: 1.4081078171730042\n",
      "keyword: foreseeable future., Score: 1.4845380187034607\n",
      "keyword: reallocation trade, Score: 2.0703224539756775\n",
      "keyword: de-risking, Score: 0.702200174331665\n",
      "keyword: months., Score: 0.6572465300559998\n",
      "keyword: pricey stock, Score: 5.932490170001984\n",
      "keyword: investor, Score: 1.2566132545471191\n",
      "keyword: law, Score: 0.678516685962677\n",
      "keyword: big number, Score: 2.469696581363678\n",
      "keyword: re big cap, Score: 3.783804774284363\n",
      "keyword: mean, Score: 0.718268871307373\n",
      "keyword: re safe, Score: 2.0050384402275085\n",
      "keyword: value stocks., Score: 1.965906798839569\n",
      "keyword: favorable macro backdrop, Score: 2.339490056037903\n",
      "keyword: strong growth demand., Score: 3.391383707523346\n",
      "keyword: run, Score: 0.6701459884643555\n",
      "keyword: artificial intelligence, Score: 2.4734750986099243\n",
      "keyword: robotics exchange-traded fund, Score: 2.8940823078155518\n",
      "keyword: asset, Score: 0.8088080286979675\n",
      "keyword: management, Score: 0.7864875793457031\n",
      "keyword: AI, Score: 1.4111855030059814\n",
      "keyword: good area, Score: 2.5309500694274902\n",
      "keyword: growth., Score: 0.7403644323348999\n",
      "keyword: ETF, Score: 0.7595927119255066\n",
      "keyword: past year, Score: 1.4395195245742798\n",
      "keyword: same span, Score: 1.336298406124115\n",
      "keyword: many point, Score: 2.477301299571991\n",
      "keyword: boost, Score: 0.6724987626075745\n",
      "keyword: sign, Score: 0.6824604272842407\n",
      "keyword: significant growth, Score: 2.6356104612350464\n",
      "keyword: FAANGs., Score: 0.7059640884399414\n",
      "keyword: percent, Score: 0.7322032451629639\n",
      "keyword: business, Score: 1.383703351020813\n",
      "keyword: 'AI, Score: 0.8195421695709229\n",
      "keyword: needle, Score: 0.8598681688308716\n",
      "keyword: business., Score: 0.8905372023582458\n",
      "keyword: 'the revenue mix, Score: 2.650937795639038\n",
      "keyword: attributable, Score: 0.8182264566421509\n",
      "keyword: insignificant., Score: 0.6884517669677734\n",
      "keyword: do, Score: 0.6344878077507019\n",
      "keyword: ecommerce, Score: 0.731623113155365\n",
      "keyword: represent, Score: 0.7921076416969299\n",
      "keyword: small portion, Score: 1.6636790037155151\n",
      "keyword: global retail market, Score: 4.451882898807526\n",
      "keyword: give, Score: 0.6857985854148865\n",
      "keyword: company room, Score: 0.6145060062408447\n"
     ]
    }
   ],
   "source": [
    "phrase_scores = []\n",
    "keywords = []\n",
    "for phrase in unique_phrases:\n",
    "    phrase_score = 0\n",
    "    keyword = ''\n",
    "    for word in phrase:\n",
    "        keyword += str(word)\n",
    "        keyword += ' '\n",
    "        phrase_score += score[vocabulary.index(word)]\n",
    "    phrase_scores.append(phrase_score)\n",
    "    keywords.append(keyword.strip())\n",
    "i=0\n",
    "print(keywords)\n",
    "for keyword in keywords:\n",
    "    print('keyword: '+str(keyword)+', Score: '+str(phrase_scores[i]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 각 구를 score로 정렬하여 핵심 구 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywords:\n",
      "\n",
      "FAANG stock win , \n",
      "FAANG stock optimist , \n",
      "FAANG stock , \n",
      "Robo Global Bill Studebaker tell Business Insider. , \n",
      "stock market , \n",
      "large-cap tech stock , \n",
      "One FAANG , \n",
      "pricey stock , \n",
      "Robo Global. Studebaker argue , \n",
      "Studebaker thinks. 'This , \n"
     ]
    }
   ],
   "source": [
    "sorted_index = np.flip(np.argsort(phrase_scores),0)\n",
    "\n",
    "keywords_num = 10\n",
    "# print(sorted_index)\n",
    "print('keywords:\\n')\n",
    "\n",
    "for i in range(0,keywords_num):\n",
    "    print(str(keywords[sorted_index[i]])+ ' , ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim Textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stocks',\n",
       " 'stock',\n",
       " 'studebaker',\n",
       " 'trade',\n",
       " 'trades',\n",
       " 'amazon',\n",
       " 'tech',\n",
       " 'attribution',\n",
       " 'attributable',\n",
       " 'cap',\n",
       " 'facebook',\n",
       " 'market',\n",
       " 'future',\n",
       " 'growth',\n",
       " 'thinks',\n",
       " 'think',\n",
       " 'frenzy',\n",
       " 'investment',\n",
       " 'big',\n",
       " 'global',\n",
       " 'favorable macro']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization import keywords\n",
    "\n",
    "keywords(Text).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The FAANG stocks won’t see much more growth in the near future, according to Bill Studebaker, founder and Chief Investment Officer of Robo Global.\n",
      "Studebaker argues we are seeing a 'reallocation' that will continue from large-cap tech stocks into market-weight stocks.\n",
      "The stock market is seeing a 'reallocation' out of FAANG stocks, which are not where the smart money is, founder and Chief Investment Officer of Robo Global Bill Studebaker told Business Insider.\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "print(summarize(Text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
