{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "spam\n",
      "1\n",
      "ham\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import queue\n",
    "\n",
    "# queue 생성\n",
    "q1 = queue.Queue()# 큐의 크기가 무한데\n",
    "q2 = queue.Queue(100)# 큐의 크기가 100\n",
    "\n",
    "#queue 자료를 넣으려면\n",
    "\n",
    "q1.put('spam')\n",
    "print(q1.qsize())\n",
    "# 큐에 데이터를 넣고, 차 있으면 대기한다\n",
    "q1.put_nowait('ham')\n",
    "print(q1.qsize())\n",
    "\n",
    "#Queue 에서 자료를 꺼내려면\n",
    "print(q1.get())\n",
    "print(q1.qsize())\n",
    "print(q1.get_nowait())\n",
    "\n",
    "#Queue 상태를 알려주는 메소드\n",
    "q1.qsize()\n",
    "q1.empty()\n",
    "q1.full()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm Threadi'm main Thread\n",
      "\n",
      "i'm main Thread\n",
      "I'm Thread\n",
      "i'm main Thread\n",
      "I'm Thread\n",
      "----=end=----\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "def my_thread(val):\n",
    "    for i in range(3):\n",
    "        print(\"I'm Thread\")\n",
    "        time.sleep(1)\n",
    "##인스턴스 만들기\n",
    "## 첫번째 argu 는 스레드 함수 이름, 두번째 argu는 매개변수를 투풀형태로 전달한 것\n",
    "t1=Thread(target = my_thread,args=(1,))\n",
    "\n",
    "##스레드 시작.\n",
    "t1.start()\n",
    "# t1.join()#t1 thread가 종료될 때까지 기다림\n",
    "\n",
    "## main\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"i'm main Thread\")\n",
    "    time.sleep(1)\n",
    "print('----=end=----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting feedparser\n",
      "  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
      "Building wheels for collected packages: feedparser\n",
      "  Building wheel for feedparser (setup.py): started\n",
      "  Building wheel for feedparser (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\82102\\AppData\\Local\\pip\\Cache\\wheels\\8c\\69\\b7\\f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
      "Successfully built feedparser\n",
      "Installing collected packages: feedparser\n",
      "Successfully installed feedparser-5.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\82102\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\82102\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\82102\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 4d5b9553-0bdf-45e7-b750-214370165ead : Watch: Journey of Hrithik's Super 30 students\n",
      ">> 6e18f14e-5590-480f-a08c-7e6f665fc479 : Batla House song Rula Diya to be unveiled tom\n",
      ">> ac0de41d-d00c-4f04-ad1f-4a6d92a7bf50 : Rahul on being charged Rs 442 for bananas\n",
      ">> af4565e0-8a13-4c52-8191-4196dc0b397c : Taimur and Inaaya's fun day out in London\n",
      ">> d82942cd-cad5-41c0-8330-2cc9d11212b7 : Female action heroes take lead in Hollywood\n",
      " << 4d5b9553-0bdf-45e7-b750-214370165ead : (GPE Watch/NN), (GPE Hrithik/NNP), \n",
      " << 6e18f14e-5590-480f-a08c-7e6f665fc479 : (PERSON Batla/NNP), (ORGANIZATION House/NNP), (PERSON Rula/NNP Diya/NNP), \n",
      " << ac0de41d-d00c-4f04-ad1f-4a6d92a7bf50 : (GPE Rahul/NN), \n",
      " << af4565e0-8a13-4c52-8191-4196dc0b397c : (GPE Taimur/NNP), (PERSON Inaaya/NNP), (GPE London/NNP), \n",
      " << d82942cd-cad5-41c0-8330-2cc9d11212b7 : (GPE Female/JJ), (GPE Hollywood/NNP), \n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "import threading\n",
    "import queue\n",
    "import feedparser\n",
    "import uuid\n",
    "threads = []\n",
    "queues = [queue.Queue(), queue.Queue()]\n",
    "def extractWords():\n",
    "    url = 'https://timesofindia.indiatimes.com/rssfeeds/1081479906.cms'\n",
    "    feed = feedparser.parse(url)\n",
    "    for entry in feed['entries'][:5]:\n",
    "        text = entry['title']\n",
    "        if 'ex' in text:\n",
    "            continue\n",
    "        words = nltk.word_tokenize(text)\n",
    "        data = {'uuid': uuid.uuid4(), 'input': words}\n",
    "        queues[0].put(data, True)\n",
    "        print(\">> {} : {}\".format(data['uuid'], text))\n",
    "def extractPOS():\n",
    "    while True:\n",
    "        if queues[0].empty():\n",
    "            break\n",
    "        else:\n",
    "            data = queues[0].get()\n",
    "            words = data['input']\n",
    "            postags = nltk.pos_tag(words)\n",
    "            queues[0].task_done()\n",
    "            queues[1].put({'uuid': data['uuid'], 'input': postags}, True)\n",
    "def extractNE():\n",
    "    while True:\n",
    "        if queues[1].empty():\n",
    "            break\n",
    "        else:\n",
    "            data = queues[1].get()\n",
    "            postags = data['input']\n",
    "            queues[1].task_done()\n",
    "            chunks = nltk.ne_chunk(postags, binary=False)\n",
    "            print(\" << {} : \".format(data['uuid']), end = '')\n",
    "            for path in chunks:\n",
    "                try:\n",
    "                    label = path.label()\n",
    "                    print(path, end=', ')\n",
    "                except:\n",
    "                    pass\n",
    "            print()\n",
    "def runProgram():\n",
    "    e = threading.Thread(target=extractWords())\n",
    "    e.start()\n",
    "    threads.append(e)\n",
    "    p = threading.Thread(target=extractPOS())\n",
    "    p.start()\n",
    "    threads.append(p)\n",
    "    n = threading.Thread(target=extractNE())\n",
    "    n.start()\n",
    "    threads.append(n)\n",
    "    queues[0].join()\n",
    "    queues[1].join()\n",
    "#print(queues[0].empty())\n",
    "#print(queues[1].empty())\n",
    "for t in threads:\n",
    "    t.join()\n",
    "if __name__ == '__main__':\n",
    "    runProgram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dic = {\n",
    "'사과' : [1.0, 0.5],\n",
    "'바나나' : [0.9, 1.2],\n",
    "'원숭이' : [0.5, 1.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def euclidean_dist(x,y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return np.sqrt(np.sum(x-y)**2)\n",
    "\n",
    "euclidean_dist(word_embedding_dic['사과'],word_embedding_dic['바나나'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'원숭이는', '좋아합니다.', '대부분', '싫어합니다.', '바나나를', '코주부'}\n",
      "{'원숭이는', '바나나를'}\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "s1 = '대부분 원숭이는 바나나를 좋아합니다.'\n",
    "s2 = '코주부 원숭이는 바나나를 싫어합니다.'\n",
    "\n",
    "token_s1 = s1.split()\n",
    "token_s2 = s2.split()\n",
    "\n",
    "union = set(token_s1).union(set(token_s2))\n",
    "print(union)\n",
    "\n",
    "\n",
    "intersection = set(token_s1).intersection(set(token_s2))\n",
    "print(intersection)\n",
    "\n",
    "print(len(intersection)/len(union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosine_similarity(x,y):\n",
    "    nominator = np.dot(x,y)\n",
    "    denominator = np.linalg.norm(x)*np.linalg.norm(y)\n",
    "    return nominator/denominator\n",
    "\n",
    "a= np.array([1,2])\n",
    "b= np.array([3,4])\n",
    "np.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_ls = [\n",
    "'오늘 동물원에서 코끼리를 봤어'\n",
    ",\n",
    "'오늘 동물원에서 원숭이에게 사과를 줬어'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['오늘', '동물원에서', '코끼리를', '봤어'], ['오늘', '동물원에서', '원숭이에게', '사과를', '줬어']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_ls = [sentence.split() for sentence in sentence_ls]\n",
    "\n",
    "sentence_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "token_dict = defaultdict(lambda : len(token_dict))\n",
    "\n",
    "for sentence in sentence_ls:\n",
    "    for token in sentence:\n",
    "        token_dict[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '오늘'),\n",
       " (1, '동물원에서'),\n",
       " (2, '코끼리를'),\n",
       " (3, '봤어'),\n",
       " (4, '원숭이에게'),\n",
       " (5, '사과를'),\n",
       " (6, '줬어')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_token_ls = sorted((value,key) for key,value in token_dict.items())\n",
    "index_token_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '동물원에서', '코끼리를', '봤어', '원숭이에게', '사과를', '줬어']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_in_order = [tup[1] for tup in index_token_ls]\n",
    "token_in_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "n_words  = len(token_dict)\n",
    "n_sentence = len(sentence_ls)\n",
    "\n",
    "BOW = pd.DataFrame(np.zeros((n_sentence,n_words)),columns = token_in_order,index = ['문장_1','문장_2'],dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,sentence in enumerate(sentence_ls):\n",
    "    for token in sentence:\n",
    "        token_location = token_dict[token]\n",
    "        BOW.iloc[i,token_location] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>오늘</th>\n",
       "      <th>동물원에서</th>\n",
       "      <th>코끼리를</th>\n",
       "      <th>봤어</th>\n",
       "      <th>원숭이에게</th>\n",
       "      <th>사과를</th>\n",
       "      <th>줬어</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>문장_1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>문장_2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      오늘  동물원에서  코끼리를  봤어  원숭이에게  사과를  줬어\n",
       "문장_1   1      1     1   1      0    0   0\n",
       "문장_2   1      1     0   0      1    1   1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
