{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "token1 = word_tokenize('Barack Obama likes freid chicken very much')\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/?ss=ai-big-data#45dd5dd61f25'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "eng_news = soup.select('p') #[class=\"speakable-paragraph\"]')\n",
    "eng_text = eng_news[3].get_text()\n",
    "\n",
    "eng_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마침표와 구두점(온점(.),컴마(,),물음표(?)등과 같은 기호)\n",
    "token1 = word_tokenize(eng_text)\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.tokenize.WordPunctTokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#알파벳과 알파벳이 아닌문자를 구분하여 토큰화\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "token2 = WordPunctTokenizer().tokenize(eng_text)\n",
    "print(token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정규표현식에 기반한 토큰화\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "token3 = TreebankWordTokenizer().tokenize(eng_text)\n",
    "print(token3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#품사 부착(Pos Tagging)\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggedToken = pos_tag(token1)\n",
    "print(taggedToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "neToken = ne_chunk(taggedToken)\n",
    "print(neToken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원형 복원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어간추출(Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print('running ->'+ps.stem('running'))\n",
    "print('beautiful ->'+ps.stem('beautiful'))\n",
    "print('believes ->'+ps.stem('believes'))\n",
    "print('using ->'+ps.stem('using'))\n",
    "print('conversation ->'+ps.stem('conversation'))\n",
    "print('organization ->'+ps.stem('organization'))\n",
    "print('studies ->'+ps.stem('studies'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 표제어 추출(Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "print('running ->' + wl.lemmatize('running'))\n",
    "print('beautiful ->' + wl.lemmatize('beautiful'))\n",
    "print('believes ->' + wl.lemmatize('believes'))\n",
    "print('using ->' + wl.lemmatize('using'))\n",
    "print('conversation ->' + wl.lemmatize('conversation'))\n",
    "print('organization ->' + wl.lemmatize('organization'))\n",
    "print('studies ->' + wl.lemmatize('studies'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불용어 처리 (Stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopPos = ['IN','CC','UH','TO','MD','DT','VBZ','VBP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(taggedToken).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWord = [',','be','able']\n",
    "word = []\n",
    "print(taggedToken)\n",
    "for tag in taggedToken:\n",
    "    if tag[1] not in stopPos:\n",
    "        if tag[0] not in stopWord:\n",
    "            word.append(tag[0])\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sumtoken = TreebankWordTokenizer().tokenize('Obama loves fried chicken of KFC')\n",
    "print(sumtoken)\n",
    "\n",
    "from nltk import pos_tag\n",
    "sumTaggedToken = pos_tag(sumtoken)\n",
    "print(sumTaggedToken)\n",
    "\n",
    "\n",
    "\n",
    "from nltk im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국어 텍스트 전처리 2019_07_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class List(list):\n",
    "    def __str__(self):\n",
    "        return '['+'. '.join(['%s' % x for x in self])+']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://news.chosun.com/site/data/html_dir/2018/07/10/2018071004121.html'\n",
    "response = requests.get(url)\n",
    "response.encoding = 'utf-8'    # 한글이므로 encoding을 utf-8로 지정\n",
    "soup = BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "kor_news = soup.select('div[class=\"par\"]')\n",
    "kor_text = kor_news[0].get_text()\n",
    "\n",
    "kor_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#코모란(komoran) 토큰화\n",
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "komoran_token = komoran.morphs(kor_text)\n",
    "print(List(komoran_token))\n",
    "\n",
    "#한나눔(Hannanum) 토큰화\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "hannanum_tokens = hannanum.morphs(kor_text)\n",
    "print(List(hannanum_tokens))\n",
    "\n",
    "#Okt 토큰화\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "okt_tokens = okt.morphs(kor_text)\n",
    "print(List(okt_tokens))\n",
    "\n",
    "#Kkma 토큰화\n",
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "kkma_tokens = kkma.morphs(kor_text)\n",
    "print(List(kkma_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 품사 부착 ( Pos Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#코모란(Komoran) 품사 태깅\n",
    "komoranTag = []\n",
    "for token in komoran_token:\n",
    "    komoranTag += komoran.pos(token)\n",
    "print(komoranTag[:10])\n",
    "\n",
    "#한나눔(Hannanum) 품사 태깅 \n",
    "hannanumTag = []\n",
    "for token in hannanum_tokens:\n",
    "    hannanumTag += hannanum.pos(token)\n",
    "print(hannanumTag[:10])\n",
    "\n",
    "#Okt 품사 태깅\n",
    "oktTag = []\n",
    "for token in okt_tokens:\n",
    "    oktTag += okt.pos(token)\n",
    "print(oktTag[:10])\n",
    "\n",
    "#Kkma 품사 태깅\n",
    "kkmaTag = []\n",
    "for token in kkma_tokens:\n",
    "    kkmaTag += kkma.pos(token)\n",
    "print(kkmaTag[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어(Stopword) 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 처리\n",
    "stopPos = ['Suffix','Punctuation','Josa','Foreign','Alpha','Number']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(oktTag).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWord = ['의','이','를','은','과','로','두고','수','했다','것','있는','한다','하는','그','있다','할','이런','되']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = []\n",
    "for tag in oktTag:\n",
    "    if tag[1] not in stopPos:\n",
    "        if tag[0] not in stopWord:\n",
    "            word.append(tag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recipe 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "files = reuters.fileids()\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words16097 = reuters.words(['test/16097'])\n",
    "print(words16097)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words20 = reuters.words(['test/16097'])[:20]\n",
    "print(words20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reutersGenres = reuters.categories()\n",
    "print(reutersGenres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in reuters.words(categories=['bop','cocoa']):\n",
    "    print(w+' ',end = '')# 다음줄로 안넘기고 공백으로 처리\n",
    "    if(w is '.'):\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import CategorizedPlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = CategorizedPlaintextCorpusReader(r'C:\\\\Users\\\\82102\\\\Desktop\\\\NLP자연어처리과정\\\\mix20_rand700_tokens_cleaned\\\\tokens',r'.*\\.txt',cat_pattern=r'(\\w+)/*')\n",
    "print(reader.categories())\n",
    "print(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posFiles = reader.fileids(categories = 'pos')\n",
    "negFiles = reader.fileids(categories = 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "fileP = posFiles[randint(0,len(posFiles)-1)]\n",
    "fileN = negFiles[randint(0,len(negFiles)-1)]\n",
    "print(fileP)\n",
    "print(fileN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in reader.words(fileP):\n",
    "    print(w+' ',end='')\n",
    "    if (w is '.'):\n",
    "        print()\n",
    "for w in reader.words(fileN):\n",
    "    print(w+' ',end = '')\n",
    "    if (w is '.'):\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "\n",
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['fiction','humor','romance']\n",
    "whwords = ['what','which','how','why','when','where','who']\n",
    "\n",
    "for i in range(0,len(genres)):\n",
    "    genre = genres[i]\n",
    "    print()\n",
    "    print('\\''+genre+'\\'wh 단어 분석')\n",
    "    genre_text = brown.words(categories = genre)\n",
    "    #빈도수를 찾을 수 있다.\n",
    "    fdist = nltk.FreqDist(genre_text)\n",
    "    for wh in whwords:\n",
    "        print(wh+':',fdist[wh],end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk,matplotlib\n",
    "nltk.download('webtext')\n",
    "from nltk.corpus import webtext\n",
    "print(webtext.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileid = 'singles.txt'\n",
    "wbt_words = webtext.words(fileid)\n",
    "fdist = nltk.FreqDist(wbt_words)\n",
    "print('최대 발생 토근\\'',fdist.max(),'\\'수 : ',fdist[fdist.max()])\n",
    "print('말뭉치 내 총 고유 토큰 수 : ',fdist.N())\n",
    "print('말뭉치에서 가장 흔한 10개 단어는 다음과 같습니다.')\n",
    "print(fdist.most_common(10))\n",
    "print('개인 광고의 빈도 분포')\n",
    "print(fdist.tabulate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.plot(cumulative = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "chair = 'chair'\n",
    "\n",
    "chair_synsets = wn.synsets(chair)\n",
    "print('의자(Chair)의 뜻 Synsets : ',chair_synsets,'\\n\\n')\n",
    "for synset in chair_synsets:\n",
    "    print(synset,':')\n",
    "    print('Definition: ', synset.definition())\n",
    "    print('Lemmas/Synonymous words: ',synset.lemma_names())\n",
    "    print('Example: ',synset.examples(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "type = 'n'\n",
    "synsets = wn.all_synsets(type)\n",
    "print(synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = []\n",
    "for synset in synsets:\n",
    "    for lemma in synset.lemmas():\n",
    "        lemmas.append(lemma.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = set(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for lemma in lemmas:\n",
    "    count = count+len(wn.synsets(lemma,type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('개별 기본형 합계:',len(lemmas))\n",
    "print('총 뜻:',count)\n",
    "print(type,'(명사)의 다의어 평균 :',count/len(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이썬에서 PDF 파일 열기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileReader\n",
    "def getTextPDF(pdfFileName,password = ''):\n",
    "    pdf_file = open(pdfFileName,'rb')\n",
    "    read_pdf = PdfFileReader(pdf_file)\n",
    "    if password !='':\n",
    "        read_pdf.decrypt(password)\n",
    "    text = []\n",
    "    for i in range(0,read_pdf.getNumPages()):\n",
    "        text.append(read_pdf.getPage(i).extractText())\n",
    "    return '\\n'.join(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfFile = 'sample-one-line.pdf' #C:\\\\Users\\\\82102\\\\Desktop\\\\NLP자연어처리과정\\\\pdf\\\\\n",
    "pdfFileEncrypted = 'sample-one-line.protected.pdf'#C:\\\\Users\\\\82102\\\\Desktop\\\\NLP자연어처리과정\\\\pdf\\\\\n",
    "print('PDF 1: \\n',getTextPDF(pdfFile))\n",
    "print('PDF 2: \\n',getTextPDF(pdfFileEncrypted,'tuffy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter 3 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import LineTokenizer, SpaceTokenizer, TweetTokenizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# lTokenizer = LineTokenizer();\n",
    "# print('Line tokenizer 출력 :'lTokenizer.tokenize('My name is Maximus Decimus Meridi'))\n",
    "lTokenizer = LineTokenizer();\n",
    "print(\"Line tokenizer 출력 : \", lTokenizer.tokenize(\"My name is Maximus Decimus Meridiu\"))\n",
    "\n",
    "\n",
    "rawText = \"By 11 o'clock on Sunday, the docor shall open the dispensary\"\n",
    "sTokenizer = SpaceTokenizer()\n",
    "print(\"Space Tokenizer 출력 :\", sTokenizer.tokenize(rawText))\n",
    "print(\"Word Tokenizer 출력 : \", word_tokenize(rawText))\n",
    "tTokenizer = TweetTokenizer()\n",
    "print(\"Tweet Tokenizer 출력 : \", tTokenizer.tokenize(\"This is a cooool #dummysmiley: :-\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer, LancasterStemmer, word_tokenize\n",
    "\n",
    "raw = 'My name is Maximus Decimus Meridius, commander of the Armies of the North, General of the Felix Legions and loyal servant to the true emperor, Marcus Aurelius. Father to a murdered son, husband to a murdered wife. And i will have my vengeance, in this like of the next.'\n",
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "pStems = [porter.stem(t) for t in tokens]\n",
    "print(pStems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "lStems = [lancaster.stem(t) for t in tokens]\n",
    "print(lStems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, PorterStemmer, WordNetLemmatizer\n",
    "raw = 'My name is Maximus Decimus Meridius, commander of the Armies of the North, General of the Felix Legions and loyal servant to the true emperor, Marcus Aurelius. Father to a murdered son, husband to a murdered wife. And i will have my vengeance, in this like of the next.'\n",
    "tokens = word_tokenize(raw)\n",
    "porter = PorterStemmer()\n",
    "stems = [porter.stem(t) for t in tokens]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "m = MeCab.Tagger()\n",
    "out = m.parse('미캅이 잘 설치되었는지 확인중입니다.')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 불용어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import gutenberg\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_words = gutenberg.words('bible-kjv.txt')\n",
    "words_filtered = [e for e in gb_words if len(e)>=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "words = [w for w in words_filtered if w.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdistPlain = nltk.FreqDist(words)\n",
    "fdist = nltk.FreqDist(gb_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Following are the most common 10 words in the bag')\n",
    "print(fdist.most_common(10))\n",
    "print('Following are the most common 10 words in the bag minus the stopwords')\n",
    "print(fdistPlain.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "def my_edit_distance(str1,str2):\n",
    "    m=len(str1)+1\n",
    "    n=len(str2)+1\n",
    "    table= {}\n",
    "    for i in range(m): table[i,0]=i\n",
    "    for j in range(n): table[0,j]=j\n",
    "        \n",
    "    for i in range(1, m):\n",
    "        for j in range(1, n):\n",
    "            cost = 0 if str1[i - 1] == str2[j - 1] else 1\n",
    "            table[i,j] = min(table[i, j-1]+1, table[i-1, j]+1, table[i-1, j-1]+cost)\n",
    "    return table[i,j]\n",
    "print(\"Our Algorithm :\",my_edit_distance(\"hand\", \"and\"))\n",
    "print(\"NLTK Algorithm :\",edit_distance(\"hand\", \"and\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story1 = \"\"\"In a far away kingdom, there was a river. This river was home to many golden swans. The swans spent most of their time on the banks of the river. Every six months, the swans would leave a golden feather as a fee for using the lake. The soldiers of the kingdom would collect the feathers and deposit them in the royal treasury. \n",
    "One day, a homeless bird saw the river. \"The water in this river seems so cool and soothing. I will make my home here,\" thought the bird. \n",
    "As soon as the bird settled down near the river, the golden swans noticed her. They came shouting. \"This river belongs to us. We pay a golden feather to the King to use this river. You can not live here.\" \n",
    "\"I am homeless, brothers. I too will pay the rent. Please give me shelter,\" the bird pleaded. \"How will you pay the rent? You do not have golden feathers,\" said the swans laughing. They further added, \"Stop dreaming and leave once.\" The humble bird pleaded many times. But the arrogant swans drove the bird away. \n",
    "\"I will teach them a lesson!\" decided the humiliated bird. \n",
    "She went to the King and said, \"O King! The swans in your river are impolite and unkind. I begged for shelter but they said that they had purchased the river with golden feathers.\" \n",
    "The King was angry with the arrogant swans for having insulted the homeless bird. He ordered his soldiers to bring the arrogant swans to his court. In no time, all the golden swans were brought to the King’s court. \n",
    "\"Do you think the royal treasury depends upon your golden feathers? You can not decide who lives by the river. Leave the river at once or you all will be beheaded!\" shouted the King. \n",
    "The swans shivered with fear on hearing the King. They flew away never to return. The bird built her home near the river and lived there happily forever. The bird gave shelter to all other birds in the river. \"\"\"\n",
    "\n",
    "story2 = \"\"\"Long time ago, there lived a King. He was lazy and liked all the comforts of life. He never carried out his duties as a King. “Our King does not take care of our needs. He also ignores the affairs of his kingdom.\" The people complained. \n",
    "One day, the King went into the forest to hunt. After having wandered for quite sometime, he became thirsty. To his relief, he spotted a lake. As he was drinking water, he suddenly saw a golden swan come out of the lake and perch on a stone. “Oh! A golden swan. I must capture it,\" thought the King. \n",
    "But as soon as he held his bow up, the swan disappeared. And the King heard a voice, “I am the Golden Swan. If you want to capture me, you must come to heaven.\" \n",
    "Surprised, the King said, “Please show me the way to heaven.\" “Do good deeds, serve your people and the messenger from heaven would come to fetch you to heaven,\" replied the voice. \n",
    "The selfish King, eager to capture the Swan, tried doing some good deeds in his Kingdom. “Now, I suppose a messenger will come to take me to heaven,\" he thought. But, no messenger came. \n",
    "The King then disguised himself and went out into the street. There he tried helping an old man. But the old man became angry and said, “You need not try to help. I am in this miserable state because of out selfish King. He has done nothing for his people.\" \n",
    "Suddenly, the King heard the golden swan’s voice, “Do good deeds and you will come to heaven.\" It dawned on the King that by doing selfish acts, he will not go to heaven. \n",
    "He realized that his people needed him and carrying out his duties was the only way to heaven. After that day he became a responsible King. \"\"\"\n",
    "\n",
    "\n",
    "story1 = story1.replace(\",\",\"\").replace(\"\\n\",\"\").replace('.','').replace('\"','').replace('!','').replace('?','').casefold()\n",
    "story2 = story2.replace(\",\",\"\").replace(\"\\n\",\"\").replace('.','').replace('\"','').replace('!','').replace('?','').casefold()\n",
    "\n",
    "story1_words = story1.split(\" \")\n",
    "print(\"첫 번째 이야기 단어 :\", story1_words)\n",
    "story2_words = story2.split(\" \")\n",
    "print(\"두 번째 이야기 단어 :\", story2_words)\n",
    "story1_vocab = set(story1_words)\n",
    "print(\"첫 번째 이야기 어휘 :\", story1_vocab)\n",
    "story2_vocab = set(story2_words)\n",
    "print(\"두 번째 이야기 어휘 :\", story2_vocab)\n",
    "common_vocab = story1_vocab & story2_vocab\n",
    "print(\"공통 어휘 :\", common_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('C:\\\\Users\\\\82102\\\\Desktop\\\\NLP자연어처리과정\\\\ekonlp\\\\eKoNLPy-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ekonlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "mecab.pos('금통위는 따라서 물가안정과 병행, 경기상황에 유의하는 금리정책을 펼쳐나가기로 했다고 밝혔다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd eKoNLPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 원핫 인코딩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np \n",
    "def one_hot_encode(word_ls):\n",
    "    #고유 단어와 인덱스를 매칭시켜주는 사전 생성\n",
    "    word2idx_dic = defaultdict(lambda:len(word2idx_dic))\n",
    "    #단어 : 인덱스 사전구축\n",
    "    for word in word_ls:\n",
    "        word2idx_dic[word]\n",
    "    n_unique_words = len(word2idx_dic)# 고유한 단어의 갯수\n",
    "    one_hot_vectors = np.zeros((len(word_ls),n_unique_words))#원 핫 벡터를 만듬\n",
    "    \n",
    "    for i,word in enumerate(word_ls):\n",
    "        index = word2idx_dic[word]#해당 단어의 고유 인덱스\n",
    "        one_hot_vectors[i,index] += 1#해당단어의 고유 인덱스에만 1을 더해줌 \n",
    "        \n",
    "    return one_hot_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encode(common_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array \n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "word_ls = ['원숭이','바나나','코끼리','사과']\n",
    "values = array(word_ls)\n",
    "print(values)\n",
    "label_enc = LabelEncoder()\n",
    "int_enc = label_enc.fit_transform(values)\n",
    "print(int_enc)\n",
    "\n",
    "onehot_enc = OneHotEncoder(sparse=False)\n",
    "int_enc = int_enc.reshape(len(int_enc),1)\n",
    "print(int_enc)\n",
    "\n",
    "onehot_enc = OneHotEncoder(sparse=False)\n",
    "onehot_enc = onehot_enc.fit_transform(int_enc)\n",
    "print(onehot_enc)\n",
    "\n",
    "imverted = label_enc.inverse_transform([argmax(onehot_enc[1,:])])\n",
    "print(imverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('금통위', 'NNG'),\n",
       " ('는', 'JX'),\n",
       " ('따라서', 'MAJ'),\n",
       " ('물가', 'NNG'),\n",
       " ('안정', 'NNG'),\n",
       " ('과', 'JC'),\n",
       " ('병행', 'NNG'),\n",
       " (',', 'SC'),\n",
       " ('경기', 'NNG'),\n",
       " ('상황', 'NNG'),\n",
       " ('에', 'JKB'),\n",
       " ('유의', 'NNG'),\n",
       " ('하', 'XSV'),\n",
       " ('는', 'ETM'),\n",
       " ('금리', 'NNG'),\n",
       " ('정책', 'NNG'),\n",
       " ('을', 'JKO'),\n",
       " ('펼쳐', 'VV'),\n",
       " ('나가', 'VX'),\n",
       " ('기', 'ETN'),\n",
       " ('로', 'JKB'),\n",
       " ('했', 'VV'),\n",
       " ('다고', 'EC'),\n",
       " ('밝혔', 'VV'),\n",
       " ('다', 'EF'),\n",
       " ('.', 'SF')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ekonlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "mecab.pos('금통위는 따라서 물가안정과 병행, 경기상황에 유의하는 금리정책을 펼쳐나가기로 했다고 밝혔다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['me free lottery' 1]\n",
      " ['free get free you' 1]\n",
      " ['you free scholarship' 0]\n",
      " ['free to contact me' 0]\n",
      " ['you won award' 0]\n",
      " ['you ticket lottery' 1]]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "k= 0.5\n",
    "\n",
    "input_file = pd.read_csv('9_spam.csv')\n",
    "training_set = np.array(input_file)\n",
    "print(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x000002033F0636A8>, {'me': [1, 1], 'free': [3, 2], 'lottery': [2, 0], 'get': [1, 0], 'you': [2, 2], 'scholarship': [0, 1], 'to': [0, 1], 'contact': [0, 1], 'won': [0, 1], 'award': [0, 1], 'ticket': [1, 0]})\n"
     ]
    }
   ],
   "source": [
    "doccnt1 = 0\n",
    "doccnt0 = 0\n",
    "\n",
    "wordfreq = defaultdict(lambda : [0,0])\n",
    "for doc,point in training_set:\n",
    "    words = doc.split()\n",
    "    for word in words:\n",
    "        if point == 1:\n",
    "            wordfreq[word][0] += 1\n",
    "        else:\n",
    "            wordfreq[word][1] += 1\n",
    "for key,(cnt1,cnt0) in wordfreq.items():\n",
    "    if cnt1 > 0:\n",
    "        doccnt1 +=1\n",
    "    if cnt0 > 0:\n",
    "        doccnt0 +=1\n",
    "print(wordfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x000002033B626A60>, {'me': [0.21428571428571427, 0.16666666666666666], 'free': [0.5, 0.2777777777777778], 'lottery': [0.35714285714285715, 0.05555555555555555], 'get': [0.21428571428571427, 0.05555555555555555], 'you': [0.35714285714285715, 0.2777777777777778], 'scholarship': [0.07142857142857142, 0.16666666666666666], 'to': [0.07142857142857142, 0.16666666666666666], 'contact': [0.07142857142857142, 0.16666666666666666], 'won': [0.07142857142857142, 0.16666666666666666], 'award': [0.07142857142857142, 0.16666666666666666], 'ticket': [0.21428571428571427, 0.05555555555555555]})\n"
     ]
    }
   ],
   "source": [
    "wordprobs = defaultdict(lambda : [0,0])\n",
    "for key,(cnt1,cnt0) in wordfreq.items():\n",
    "    wordprobs[key][0] = (cnt1+k)/(doccnt1 +2*k)\n",
    "    wordprobs[key][1] = (cnt0+k)/(doccnt0 +2*k)\n",
    "print(wordprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good price\n",
      "스팸확률 : 26.071196249739348\n",
      "정상확률 : 73.92880375026067\n"
     ]
    }
   ],
   "source": [
    "doc = 'good price'\n",
    "tokens = doc.split()\n",
    "\n",
    "log_prob1 = log_prob0 = 0.0\n",
    "\n",
    "for word,(prob1,prob0) in wordprobs.items():\n",
    "    if word in tokens:\n",
    "        log_prob1 += math.log(prob1)\n",
    "        log_prob0 += math.log(prob0)\n",
    "    else:\n",
    "        log_prob1 += math.log(1.0-prob1)\n",
    "        log_prob0 += math.log(1.0-prob0)\n",
    "log_prob1 += math.log(doccnt1/len(wordfreq))\n",
    "log_prob0 += math.log(doccnt0/len(wordfreq))\n",
    "\n",
    "prob1 = math.exp(log_prob1)\n",
    "prob0 = math.exp(log_prob0)\n",
    "\n",
    "print(doc)\n",
    "print(f'스팸확률 : {prob1/(prob1+prob0)*100}')\n",
    "print(f'정상확률 : {prob0/(prob1+prob0)*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
